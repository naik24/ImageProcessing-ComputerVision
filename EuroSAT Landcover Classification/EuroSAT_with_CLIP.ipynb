{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA4ClGtRAX40"
      },
      "source": [
        "## Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awoJxLpAduAP",
        "outputId": "7fc7c750-1337-402e-ce74-5a0c2edcf415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDAIhc0tAPFN",
        "outputId": "e7ba7a2f-e4f5-49dc-d66e-052e29260010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch==1.7.1 (from versions: 0.1.2, 1.0.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch==1.7.1\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fuxlfutv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-fuxlfutv\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.18.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=a1fc0741ea22087833f00fa1dc4bcd7f59d83b12e672bbc56620db0359810e9f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h5cplxbp/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch==1.7.1 torchvision\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnVPPtouBZdl"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wgBPs_q5BbUn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krc-AzzqlLcO"
      },
      "source": [
        "# Running Inference on Pre-Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-L8kMU6BnKU"
      },
      "source": [
        "## Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "diDKMNPgBoBk"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3fGr372Bqgs"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OqEcgQl-B5_X"
      },
      "outputs": [],
      "source": [
        "classes = [\n",
        "    'forest',\n",
        "    'permanent crop land',\n",
        "    'residential buildings or homes or apartments',\n",
        "    'river',\n",
        "    'pasture land',\n",
        "    'lake or sea',\n",
        "    'brushland or shrubland',\n",
        "    'annual crop land',\n",
        "    'industrial buildings or commercial buildings',\n",
        "    'highway or road',\n",
        "]\n",
        "\n",
        "dataPath = os.path.join(os.getcwd(), \"test\")\n",
        "images = []\n",
        "groundTruth_labels = []\n",
        "\n",
        "for cls in classes:\n",
        "  for img in os.listdir(os.path.join(dataPath, cls)):\n",
        "    images.append(os.path.join(dataPath, cls, img))\n",
        "    groundTruth_labels.append(f\"a centered satellite photo of {cls}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t-9sm7MLCGlz"
      },
      "outputs": [],
      "source": [
        "# Function to analyze an image\n",
        "def analyze_image(image_path, descriptions):\n",
        "    # Preprocess the image\n",
        "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode the descriptions\n",
        "    text = clip.tokenize(descriptions).to(device)\n",
        "\n",
        "    # Get the image and text features\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    # Calculate similarity\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (image_features @ text_features.T).squeeze()\n",
        "\n",
        "    # Find the description with the highest similarity\n",
        "    best_match_idx = similarity.argmax().item()\n",
        "    best_description = descriptions[best_match_idx]\n",
        "    return best_description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j7kUMW_CHqb",
        "outputId": "0392d278-2ef8-4ab7-97a2-7d7f32a1cab9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing images: 100%|██████████| 5000/5000 [01:48<00:00, 46.03it/s]\n"
          ]
        }
      ],
      "source": [
        "preTrained_predictions = []\n",
        "for img in tqdm(images, desc=\"Analyzing images\"):\n",
        "  cls = analyze_image(img, classes)\n",
        "  preTrained_predictions.append(f\"a centered satellite photo of {cls}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgIbQp5ZCV8i"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYv_j0RJCWpW",
        "outputId": "56151d40-ba55-4e62-90fc-4a3663251936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4218\n"
          ]
        }
      ],
      "source": [
        "acc = accuracy_score(preTrained_predictions, groundTruth_labels)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4oCjafxCtXv"
      },
      "source": [
        "# Fine-Tuning CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuXW-jiReR5H",
        "outputId": "ba7b5443-b36b-4ec8-b607-25dadd1eef8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 104MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# loading model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BlQhrWAiWJtq"
      },
      "outputs": [],
      "source": [
        "# defining custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, images, descriptions):\n",
        "    self.images = images\n",
        "    self.descriptions = clip.tokenize(descriptions)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = preprocess(Image.open(self.images[idx]))\n",
        "    description = self.descriptions[idx]\n",
        "    return image, description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mLNXxcXTYu80"
      },
      "outputs": [],
      "source": [
        "# defining descriptions\n",
        "descriptions = [\"annual crop land\",\n",
        "                \"forest\",\n",
        "                \"lake or sea\",\n",
        "                \"pasture land\",\n",
        "                \"permanent crop land\",\n",
        "                \"river\",\n",
        "                \"residential buildings or homes or apartments\",\n",
        "                \"industrial buildings or commercial buildings\",\n",
        "                \"highway or road\",\n",
        "                \"brushland or shrubland\"]\n",
        "\n",
        "# preparing dataset\n",
        "training_path = os.path.join(os.getcwd(), \"train\")\n",
        "\n",
        "training_images = []\n",
        "training_descriptions = []\n",
        "\n",
        "for cls in descriptions:\n",
        "  for img in os.listdir(os.path.join(training_path, cls)):\n",
        "    training_images.append(os.path.join(training_path, cls, img))\n",
        "    training_descriptions.append(f\"a centered satellite photo of {cls}\")\n",
        "\n",
        "training_dataset = CustomDataset(training_images, training_descriptions)\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IOuE9qPXZSZf"
      },
      "outputs": [],
      "source": [
        "# function to convert model's parameters to FP32 format\n",
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2AUlmjDFZk29"
      },
      "outputs": [],
      "source": [
        "# defining optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, betas=(0.9,0.98), eps=1e-6, weight_decay=0.2)\n",
        "\n",
        "# loss functions\n",
        "loss_img = torch.nn.CrossEntropyLoss()\n",
        "loss_descriptions = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XUEf8giZx-R",
        "outputId": "13f9d5fd-97a1-439f-a266-a00e3ceb87cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0/16, Loss: 2.2051: 100%|██████████| 688/688 [02:11<00:00,  5.24it/s]\n",
            "Epoch 1/16, Loss: 2.1951: 100%|██████████| 688/688 [02:09<00:00,  5.32it/s]\n",
            "Epoch 2/16, Loss: 2.1397: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n",
            "Epoch 3/16, Loss: 2.0760: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n",
            "Epoch 4/16, Loss: 2.0547: 100%|██████████| 688/688 [02:08<00:00,  5.34it/s]\n",
            "Epoch 5/16, Loss: 2.0288: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n",
            "Epoch 6/16, Loss: 2.0133: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n",
            "Epoch 7/16, Loss: 2.0014: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n",
            "Epoch 8/16, Loss: 1.9903: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n",
            "Epoch 9/16, Loss: 1.9782: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n",
            "Epoch 10/16, Loss: 1.9664: 100%|██████████| 688/688 [02:08<00:00,  5.34it/s]\n",
            "Epoch 11/16, Loss: 1.9684: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n",
            "Epoch 12/16, Loss: 1.9593: 100%|██████████| 688/688 [02:08<00:00,  5.34it/s]\n",
            "Epoch 13/16, Loss: 1.9545: 100%|██████████| 688/688 [02:08<00:00,  5.34it/s]\n",
            "Epoch 14/16, Loss: 1.9608: 100%|██████████| 688/688 [02:08<00:00,  5.34it/s]\n",
            "Epoch 15/16, Loss: 1.9686: 100%|██████████| 688/688 [02:08<00:00,  5.35it/s]\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "num_epochs = 16\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  pbar = tqdm(training_dataloader, total = len(training_dataloader))\n",
        "  #for images, texts in training_dataloader:\n",
        "  for batch in pbar:\n",
        "    # zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    images, texts = batch\n",
        "    images = images.to(device)\n",
        "    texts = texts.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "    # compute loss\n",
        "    ground_truth = torch.arange(len(images), dtype = torch.long, device = device)\n",
        "    loss = (loss_img(logits_per_image, ground_truth) + loss_descriptions(logits_per_text, ground_truth)) / 2\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    if device == \"cpu\":\n",
        "      optimizer.step()\n",
        "    else:\n",
        "      convert_models_to_fp32(model)\n",
        "      optimizer.step()\n",
        "      clip.model.convert_weights(model)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(training_dataloader)\n",
        "    pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZTLG7-4MwuvO"
      },
      "outputs": [],
      "source": [
        "# saving the model\n",
        "torch.save(model.state_dict(), \"euroSATclip.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGF9EpM_xynG",
        "outputId": "0c1b267c-2743-483c-b030-a4d3737b56d5"
      },
      "outputs": [],
      "source": [
        "# running predictions on test set\n",
        "!unzip test.zip\n",
        "\n",
        "classes = [\n",
        "    'forest',\n",
        "    'permanent crop land',\n",
        "    'residential buildings or homes or apartments',\n",
        "    'river',\n",
        "    'pasture land',\n",
        "    'lake or sea',\n",
        "    'brushland or shrubland',\n",
        "    'annual crop land',\n",
        "    'industrial buildings or commercial buildings',\n",
        "    'highway or road',\n",
        "]\n",
        "\n",
        "dataPath = os.path.join(os.getcwd(), \"test\")\n",
        "testImages = []\n",
        "groundTruth_Testlabels = []\n",
        "\n",
        "for cls in classes:\n",
        "  for img in os.listdir(os.path.join(dataPath, cls)):\n",
        "    testImages.append(os.path.join(dataPath, cls, img))\n",
        "    groundTruth_Testlabels.append(f\"a centered satellite photo of {cls}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HrakCfy8yqhY"
      },
      "outputs": [],
      "source": [
        "# Function to analyze an image\n",
        "def analyze_image(image_path, descriptions):\n",
        "    # Preprocess the image\n",
        "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode the descriptions\n",
        "    text = clip.tokenize(descriptions).to(device)\n",
        "\n",
        "    # Get the image and text features\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    # Calculate similarity\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (image_features @ text_features.T).squeeze()\n",
        "\n",
        "    # Find the description with the highest similarity\n",
        "    best_match_idx = similarity.argmax().item()\n",
        "    best_description = descriptions[best_match_idx]\n",
        "    return best_description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy2eelgFytZi",
        "outputId": "17723e98-2442-4ed9-c539-35337cdb329d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing images: 100%|██████████| 5000/5000 [01:47<00:00, 46.35it/s]\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"euroSATclip.pt\"))\n",
        "\n",
        "fineTuned_predictions = []\n",
        "for img in tqdm(testImages, desc=\"Analyzing images\"):\n",
        "  cls = analyze_image(img, classes)\n",
        "  fineTuned_predictions.append(f\"a centered satellite photo of {cls}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_6tGSea0Qzw",
        "outputId": "b9f848c2-fc18-4e0f-ac59-1bbfac2a7412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7376\n"
          ]
        }
      ],
      "source": [
        "acc = accuracy_score(fineTuned_predictions, groundTruth_labels)\n",
        "print(acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xA4ClGtRAX40",
        "u-L8kMU6BnKU",
        "bgIbQp5ZCV8i"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
